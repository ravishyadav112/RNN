{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1e51d7-4e1f-486f-b169-2f239c3bd8d0",
   "metadata": {},
   "source": [
    "Implementation of a Recurrent Neural Network (RNN) for next-word prediction . The code includes:\n",
    "\n",
    "A complete NextWordPredictor class that handles:\n",
    "\n",
    "1- Text preprocessing and tokenization\n",
    "2- Data preparation and sequence creation\n",
    "3- Building an RNN model with embedding, SimpleRNN, and dense layers\n",
    "4- Training functionality\n",
    "5- Prediction capabilities for the next word given an input sentence\n",
    "6- Model saving and loading\n",
    "\n",
    "\n",
    "Example usage showing how to:\n",
    "\n",
    "Train the model on sample texts\n",
    "Make predictions on test sentences\n",
    "\n",
    "The implementation uses TensorFlow and Keras, which are popular frameworks for building neural networks. This code demonstrates how RNNs can process sequences of words and predict what might come next based on the patterns learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897bdcf3-429e-4d71-9241-59612b894197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Downloads\\ananconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.0145 - loss: 6.9084 - val_accuracy: 0.0000e+00 - val_loss: 6.8684\n",
      "Epoch 2/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1074 - loss: 6.4985 - val_accuracy: 0.0000e+00 - val_loss: 6.2290\n",
      "Epoch 3/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0319 - loss: 4.9801 - val_accuracy: 0.0000e+00 - val_loss: 6.1732\n",
      "Epoch 4/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0742 - loss: 3.9750 - val_accuracy: 0.0000e+00 - val_loss: 6.7417\n",
      "Epoch 5/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0623 - loss: 3.8831 - val_accuracy: 0.0000e+00 - val_loss: 7.0937\n",
      "Epoch 6/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0304 - loss: 3.6798 - val_accuracy: 0.0000e+00 - val_loss: 7.2832\n",
      "Epoch 7/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0340 - loss: 3.7349 - val_accuracy: 0.0000e+00 - val_loss: 7.3981\n",
      "Epoch 8/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0934 - loss: 3.6739 - val_accuracy: 0.0000e+00 - val_loss: 7.5435\n",
      "Epoch 9/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0920 - loss: 3.6913 - val_accuracy: 0.0000e+00 - val_loss: 7.5550\n",
      "Epoch 10/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0506 - loss: 3.7108 - val_accuracy: 0.0000e+00 - val_loss: 7.6478\n",
      "Epoch 11/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0751 - loss: 3.7439 - val_accuracy: 0.0000e+00 - val_loss: 7.6952\n",
      "Epoch 12/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0101 - loss: 3.7535 - val_accuracy: 0.0000e+00 - val_loss: 7.7520\n",
      "Epoch 13/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0488 - loss: 3.7091 - val_accuracy: 0.0667 - val_loss: 7.8131\n",
      "Epoch 14/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1495 - loss: 3.6082 - val_accuracy: 0.0667 - val_loss: 7.8352\n",
      "Epoch 15/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0873 - loss: 3.6528 - val_accuracy: 0.0000e+00 - val_loss: 7.8888\n",
      "Epoch 16/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0386 - loss: 3.6835 - val_accuracy: 0.0000e+00 - val_loss: 7.9180\n",
      "Epoch 17/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0411 - loss: 3.7036 - val_accuracy: 0.0000e+00 - val_loss: 7.9052\n",
      "Epoch 18/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0659 - loss: 3.6888 - val_accuracy: 0.0667 - val_loss: 7.9536\n",
      "Epoch 19/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0800 - loss: 3.6950 - val_accuracy: 0.0667 - val_loss: 7.9425\n",
      "Epoch 20/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0646 - loss: 3.7287 - val_accuracy: 0.0000e+00 - val_loss: 7.9398\n",
      "Epoch 21/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1192 - loss: 3.6312 - val_accuracy: 0.0000e+00 - val_loss: 7.8770\n",
      "Epoch 22/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0692 - loss: 3.7026 - val_accuracy: 0.0000e+00 - val_loss: 7.8539\n",
      "Epoch 23/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0570 - loss: 3.7566 - val_accuracy: 0.0000e+00 - val_loss: 7.8961\n",
      "Epoch 24/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1385 - loss: 3.5523 - val_accuracy: 0.0000e+00 - val_loss: 7.7436\n",
      "Epoch 25/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1831 - loss: 3.6267 - val_accuracy: 0.0667 - val_loss: 7.7063\n",
      "Epoch 26/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2307 - loss: 3.4706 - val_accuracy: 0.0667 - val_loss: 7.6591\n",
      "Epoch 27/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1336 - loss: 3.4862 - val_accuracy: 0.0000e+00 - val_loss: 7.5465\n",
      "Epoch 28/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2374 - loss: 3.2901 - val_accuracy: 0.0000e+00 - val_loss: 7.3921\n",
      "Epoch 29/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1919 - loss: 3.3683 - val_accuracy: 0.0667 - val_loss: 6.5275\n",
      "Epoch 30/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2747 - loss: 3.1940 - val_accuracy: 0.0000e+00 - val_loss: 7.7475\n",
      "Epoch 31/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2786 - loss: 3.1215 - val_accuracy: 0.0667 - val_loss: 7.5973\n",
      "Epoch 32/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2792 - loss: 2.9883 - val_accuracy: 0.0000e+00 - val_loss: 7.1741\n",
      "Epoch 33/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4761 - loss: 2.6694 - val_accuracy: 0.0667 - val_loss: 6.9818\n",
      "Epoch 34/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3566 - loss: 2.8526 - val_accuracy: 0.0667 - val_loss: 7.0492\n",
      "Epoch 35/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3219 - loss: 2.6863 - val_accuracy: 0.0667 - val_loss: 6.9535\n",
      "Epoch 36/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5146 - loss: 2.4806 - val_accuracy: 0.0667 - val_loss: 6.8107\n",
      "Epoch 37/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6806 - loss: 2.2172 - val_accuracy: 0.1333 - val_loss: 6.9172\n",
      "Epoch 38/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6110 - loss: 2.2185 - val_accuracy: 0.1333 - val_loss: 6.9590\n",
      "Epoch 39/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7836 - loss: 1.9026 - val_accuracy: 0.1333 - val_loss: 6.7445\n",
      "Epoch 40/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8252 - loss: 1.9732 - val_accuracy: 0.1333 - val_loss: 6.9708\n",
      "Epoch 41/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8461 - loss: 1.8089 - val_accuracy: 0.1333 - val_loss: 6.7649\n",
      "Epoch 42/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9194 - loss: 1.5740 - val_accuracy: 0.1333 - val_loss: 6.8382\n",
      "Epoch 43/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8601 - loss: 1.5573 - val_accuracy: 0.1333 - val_loss: 6.7786\n",
      "Epoch 44/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9027 - loss: 1.3763 - val_accuracy: 0.2000 - val_loss: 6.8281\n",
      "Epoch 45/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9033 - loss: 1.3235 - val_accuracy: 0.1333 - val_loss: 6.7954\n",
      "Epoch 46/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8975 - loss: 1.1645 - val_accuracy: 0.1333 - val_loss: 6.7722\n",
      "Epoch 47/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8912 - loss: 1.0983 - val_accuracy: 0.1333 - val_loss: 6.8123\n",
      "Epoch 48/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9377 - loss: 0.9848 - val_accuracy: 0.2000 - val_loss: 6.7071\n",
      "Epoch 49/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8698 - loss: 0.8753 - val_accuracy: 0.2000 - val_loss: 6.7554\n",
      "Epoch 50/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9145 - loss: 0.8564 - val_accuracy: 0.3333 - val_loss: 6.6522\n",
      "\n",
      "Prediction examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Input: 'machine learning is'\n",
      "Predicted next words: ['a', 'uses', 'learning']\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Input: 'deep learning uses neural'\n",
      "Predicted next words: ['networks', 'subset', 'is']\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Input: 'recurrent neural networks can process'\n",
      "Predicted next words: ['to', 'layers', 'systems']\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Input: 'natural language processing helps computers'\n",
      "Predicted next words: ['subset', 'field', 'neural']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import string\n",
    "\n",
    "class NextWordPredictor:\n",
    "    def __init__(self, vocab_size=5000, embedding_dim=100, rnn_units=150, max_sequence_length=20):\n",
    "        \"\"\"\n",
    "        Initialize the Next Word Predictor with RNN\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Maximum number of words in vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            rnn_units: Number of units in the RNN layer\n",
    "            max_sequence_length: Maximum length of input sequences\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text by removing punctuation and making lowercase\"\"\"\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator).lower()\n",
    "        return text\n",
    "    \n",
    "    def create_sequences(self, text):\n",
    "        \"\"\"Create input sequences and output words for training\"\"\"\n",
    "        words = text.split()\n",
    "        sequences = []\n",
    "        for i in range(1, len(words)):\n",
    "            seq_words = words[max(0, i-self.max_sequence_length):i]\n",
    "            sequences.append([' '.join(seq_words), words[i]])\n",
    "        return sequences\n",
    "    \n",
    "    def prepare_data(self, texts):\n",
    "        \"\"\"Prepare training data from texts\"\"\"\n",
    "        # Preprocess all texts\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Create sequences\n",
    "        all_sequences = []\n",
    "        for text in processed_texts:\n",
    "            all_sequences.extend(self.create_sequences(text))\n",
    "        \n",
    "        # Split into inputs and targets\n",
    "        input_sequences = [seq[0] for seq in all_sequences]\n",
    "        target_words = [seq[1] for seq in all_sequences]\n",
    "        \n",
    "        # Fit tokenizer on all input sequences and target words\n",
    "        all_text = ' '.join(input_sequences + target_words)\n",
    "        self.tokenizer.fit_on_texts([all_text])\n",
    "        \n",
    "        # Convert input sequences to token sequences\n",
    "        X = self.tokenizer.texts_to_sequences(input_sequences)\n",
    "        X_padded = pad_sequences(X, maxlen=self.max_sequence_length, padding='pre')\n",
    "        \n",
    "        # Convert target words to one-hot encoded vectors\n",
    "        y = self.tokenizer.texts_to_sequences(target_words)\n",
    "        y = np.array([seq[0] if seq else 0 for seq in y])\n",
    "        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)\n",
    "        \n",
    "        return X_padded, y\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the RNN model for next word prediction\"\"\"\n",
    "        self.model = Sequential([\n",
    "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_sequence_length),\n",
    "            SimpleRNN(self.rnn_units, return_sequences=False),\n",
    "            Dense(self.vocab_size, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, texts, epochs=10, batch_size=64, validation_split=0.2):\n",
    "        \"\"\"Train the model on the provided texts\"\"\"\n",
    "        X, y = self.prepare_data(texts)\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict_next_word(self, input_text, num_predictions=3):\n",
    "        \"\"\"Predict the next word for the given input text\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "        \n",
    "        # Preprocess input text\n",
    "        processed_text = self.preprocess_text(input_text)\n",
    "        \n",
    "        # Convert to sequence\n",
    "        token_sequence = self.tokenizer.texts_to_sequences([processed_text])[0]\n",
    "        \n",
    "        # Pad sequence\n",
    "        padded_sequence = pad_sequences([token_sequence], maxlen=self.max_sequence_length, padding='pre')\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(padded_sequence)[0]\n",
    "        top_indices = predictions.argsort()[-num_predictions:][::-1]\n",
    "        \n",
    "        # Convert indices to words\n",
    "        index_to_word = {idx: word for word, idx in self.tokenizer.word_index.items()}\n",
    "        predicted_words = [index_to_word.get(idx, \"<Unknown>\") for idx in top_indices if idx != 0]\n",
    "        \n",
    "        return predicted_words\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to a file\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save\")\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load the model from a file\"\"\"\n",
    "        self.model = tf.keras.models.load_model(filepath)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample text for training\n",
    "    sample_texts = [\n",
    "        \"Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience\",\n",
    "        \"Deep learning is a subset of machine learning that uses neural networks with many layers\",\n",
    "        \"Recurrent neural networks are designed to recognize patterns in sequences of data such as text time series or speech\",\n",
    "        \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize the predictor\n",
    "    predictor = NextWordPredictor(vocab_size=1000, embedding_dim=64, rnn_units=100, max_sequence_length=10)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    predictor.train(sample_texts, epochs=50, batch_size=4)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_sentences = [\n",
    "        \"machine learning is\",\n",
    "        \"deep learning uses neural\",\n",
    "        \"recurrent neural networks can process\",\n",
    "        \"natural language processing helps computers\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPrediction examples:\")\n",
    "    for sentence in test_sentences:\n",
    "        next_words = predictor.predict_next_word(sentence)\n",
    "        print(f\"Input: '{sentence}'\")\n",
    "        print(f\"Predicted next words: {next_words}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f31b4c-de53-443d-ae8d-2c3a8ffcefdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RNN Next Word Prediction: Simple Explanation\n",
    "\n",
    "## What This Code Does\n",
    "\n",
    "Imagine you're playing a word guessing game where your friend says part of a sentence and you have to guess what word comes next. For example, if your friend says \"I want to eat a...\" you might guess \"sandwich\" or \"pizza\". Our computer program does the same thing - it tries to predict what word should come next in a sentence.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "### The Big Picture\n",
    "1. We feed a bunch of sentences to the computer.\n",
    "2. The computer learns patterns about which words usually follow other words.\n",
    "3. When you give it a new sentence, it can guess what word might come next.\n",
    "\n",
    "### Key Parts of the Code Explained\n",
    "\n",
    "#### 1. NextWordPredictor Class\n",
    "This is like a robot that we build and train to guess the next word. Just like you need to learn vocabulary before writing essays, our robot needs to know about words before making predictions.\n",
    "\n",
    "```python\n",
    "class NextWordPredictor:\n",
    "    def __init__(self, vocab_size=5000, embedding_dim=100, rnn_units=150, max_sequence_length=20):\n",
    "```\n",
    "\n",
    "- `vocab_size`: The maximum number of words our robot can learn (its vocabulary)\n",
    "- `embedding_dim`: How the robot remembers each word (like assigning special codes to words)\n",
    "- `rnn_units`: How smart our robot is (more units = smarter, but slower)\n",
    "- `max_sequence_length`: How many previous words it looks at to make a prediction\n",
    "\n",
    "#### 2. Data Preparation\n",
    "Before training, we need to prepare our sentences in a way the computer can understand:\n",
    "\n",
    "```python\n",
    "def preprocess_text(self, text):\n",
    "    \"\"\"Remove punctuation and make lowercase\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator).lower()\n",
    "    return text\n",
    "```\n",
    "\n",
    "This is like cleaning up messy handwriting so it's easier to read. We remove punctuation (like periods and commas) and make all letters lowercase.\n",
    "\n",
    "```python\n",
    "def create_sequences(self, text):\n",
    "    \"\"\"Create input sequences and output words for training\"\"\"\n",
    "```\n",
    "\n",
    "This splits our sentences into pairs of \"what we show the robot\" and \"what we want it to guess\". For example, from \"I love to play basketball with my friends\", we might create:\n",
    "- Input: \"I love to play\" → Output: \"basketball\"\n",
    "- Input: \"love to play basketball\" → Output: \"with\"\n",
    "- And so on...\n",
    "\n",
    "#### 3. The Brain (RNN Model)\n",
    "This is where the magic happens! We build a brain for our robot:\n",
    "\n",
    "```python\n",
    "def build_model(self):\n",
    "    \"\"\"Build the RNN model for next word prediction\"\"\"\n",
    "    self.model = Sequential([\n",
    "        Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_sequence_length),\n",
    "        SimpleRNN(self.rnn_units, return_sequences=False),\n",
    "        Dense(self.vocab_size, activation='softmax')\n",
    "    ])\n",
    "```\n",
    "\n",
    "The brain has three main parts:\n",
    "- `Embedding`: Converts words into numbers the computer can understand\n",
    "- `SimpleRNN`: The memory part that remembers patterns in word sequences\n",
    "- `Dense`: Makes the final decision about which word comes next\n",
    "\n",
    "#### 4. Training\n",
    "Just like you learn by studying examples, our robot learns by studying lots of sentences:\n",
    "\n",
    "```python\n",
    "def train(self, texts, epochs=10, batch_size=64, validation_split=0.2):\n",
    "```\n",
    "\n",
    "- `texts`: All the sentences we use for training\n",
    "- `epochs`: How many times the robot studies all the sentences\n",
    "- `batch_size`: How many examples it looks at before updating what it's learned\n",
    "- `validation_split`: Part of the data used to check how well it's learning\n",
    "\n",
    "#### 5. Making Predictions\n",
    "After training, the robot can predict the next word in new sentences:\n",
    "\n",
    "```python\n",
    "def predict_next_word(self, input_text, num_predictions=3):\n",
    "```\n",
    "\n",
    "This function:\n",
    "1. Takes your sentence\n",
    "2. Cleans it up (removes punctuation, etc.)\n",
    "3. Converts it to numbers the robot understands\n",
    "4. Uses the trained brain to predict the most likely next words\n",
    "5. Returns the top few guesses\n",
    "\n",
    "## Example in Action\n",
    "\n",
    "```python\n",
    "# Initialize the predictor\n",
    "predictor = NextWordPredictor(vocab_size=1000, embedding_dim=64, rnn_units=100, max_sequence_length=10)\n",
    "\n",
    "# Train the model\n",
    "predictor.train(sample_texts, epochs=50, batch_size=4)\n",
    "\n",
    "# Make predictions\n",
    "test_sentences = [\"machine learning is\"]\n",
    "next_words = predictor.predict_next_word(test_sentences[0])\n",
    "print(f\"Input: '{test_sentences[0]}'\")\n",
    "print(f\"Predicted next words: {next_words}\")\n",
    "```\n",
    "\n",
    "If we train it on science tThe difference between this simple version and the advanced ones is mostly just how big and complex the \"brain\" part is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0d6e0-d2ac-4d98-b69f-a195ea9125d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredictor:\n",
    "    def __init__(self, vocab_size=5000, embedding_dim=100, rnn_units=150, max_sequence_length=20):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
